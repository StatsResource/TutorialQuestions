
%------------------------------------------------------------------- %
% Question 5
% Huffman Coding
%------------------------------------------------------------------- %
\newpage
\subsection*{Question 5 [25 marks] }
\begin{itemize}
\item[(a)] \textbf{\textit{Huffman Coding (8 Marks)}}\\
A discrete memoryless source $X$ has five symbols $\{x_1,x_2,x_3,x_4,x_5\}$ with probabilities $P(x_1) = 0.45$ , $P(x_2) = 0.20$, $P(x_3) = 0.16$, $P(x_4) = 0.14$ and $P(x_5) = 0.05$.

\begin{itemize}
\item (5 Marks) Construct a Huffman code for X.
\item  Calculate the efficiency of the code.
%\item[(iii)] (1 marks) Calculate the redundancy of the code.
\end{itemize}
\bigskip
\item[(b)] 

{
\normalsize
\textit{\textbf{Please turn over for the remaining sections of Question 5.}}
}
%--------------------------------------------------------%
%  Question 5

%  Entropy
%  Huffman Coding
%--------------------------------------------------------%
\newpage
%\end{itemize}
\begin{itemize}
\item[(c)]


\item[(d)] \textbf{\textit{Communication Channels (4 Marks)}}\\
The input source to a noisy communication channel is a random variable X over the
four symbols $\{a, b, c, d\}$. The output from this channel is a random variable Y over these same
four symbols. \\
\vspace{0.3cm}
\noindent 
The joint distribution of these two random variables is as follows:\\ \bigskip

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
&x=a& x=b & x=c & x=d \\ \hline
y=a &1/8 &0 &0 & 0 \\ \hline
y=b &0 & 1/4& 1/8& 0 \\ \hline
y=c & 0&1/16 & 1/8 & 0\\ \hline
y=d & 1/16& 0& 0 & 1/4\\ \hline
\end{tabular}
\end{center}

\begin{itemize}
\item  Write down the marginal distribution for $X$ and compute the marginal entropy $H(X)$.
\item  Write down the marginal distribution for $Y$ and compute the marginal entropy $H(Y )$.
%\item[(iii)]  What is the joint entropy $H(X, Y ) $ of the two random variables?
%\item[(iv)] (4 marks) What is the conditional entropy $H(Y|X)$?
%\item[(v)] (3 marks) What is the conditional entropy $H(X|Y)$?
%\item[(vi)] (3 marks) What is the mutual information $I(X;Y)$ between the two random variables?
\end{itemize}

\end{itemize}
\newpage


